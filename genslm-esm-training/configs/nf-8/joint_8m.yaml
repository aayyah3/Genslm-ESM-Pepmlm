per_device_train_batch_size: 4
per_device_eval_batch_size: 8
gradient_accumulation_steps: 16
num_train_epochs: 50
compute_codon_loss: true
compute_aminoacid_loss: true
compute_contrastive_loss: false
contrastive_temperature: 0.1
contrastive_pooler: mean
base_model: facebook/esm2_t6_8M_UR50D
tokenizer_path: tokenizer_esm_genslm
output_path: nf_production_runs/nf8_joint_8m
train_path: /lambda_stor/homes/khippe/genslm_foundation/genome_data/curriculum_datasets/curriculum_8/curriculum_8_train.h5
validation_path: /lambda_stor/homes/khippe/genslm_foundation/genome_data/curriculum_datasets/curriculum_8/curriculum_8_val.h5
wandb_project: genslm-esm-nf-experiments
