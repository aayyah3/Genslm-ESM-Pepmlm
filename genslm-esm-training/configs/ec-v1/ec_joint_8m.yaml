training_args:
  output_dir: runs/ec_v1/ec_joint_8m
  num_train_epochs: 20
  per_device_train_batch_size: 16
  per_device_eval_batch_size: 16
  gradient_accumulation_steps: 2
  dataloader_num_workers: 4
  eval_steps: 500
  logging_steps: 500
  save_steps: 500

compute_codon_loss: true
compute_aminoacid_loss: true
compute_contrastive_loss: false
contrastive_temperature: 0.1
contrastive_pooler: mean
base_model: facebook/esm2_t6_8M_UR50D
tokenizer_path: tokenizer_esm_genslm
train_path: examples/ec/ec_data_v1/ec_v1_train.fasta
eval_path: examples/ec/ec_data_v1/ec_v1_valid.fasta
wandb_project: genslm-esm-ec-experiments
