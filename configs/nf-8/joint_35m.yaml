per_device_train_batch_size: 32
per_device_eval_batch_size: 64
gradient_accumulation_steps: 4
num_train_epochs: 50
compute_codon_loss: true
compute_aminoacid_loss: true
compute_contrastive_loss: false
contrastive_temperature: 0.1
contrastive_pooler: mean
base_model: facebook/esm2_t12_35M_UR50D
tokenizer_path: tokenizer_esm_genslm
output_path: nf_production_runs/nf8_joint_35m
train_path: /lambda_stor/homes/khippe/genslm_foundation/genome_data/curriculum_datasets/curriculum_8/curriculum_8_train.h5
validation_path: /lambda_stor/homes/khippe/genslm_foundation/genome_data/curriculum_datasets/curriculum_8/curriculum_8_val.h5
wandb_project: genslm-esm