training_args:
  output_dir: /lus/eagle/projects/CVD-Mol-AI/braceal/src/genslm-esm/runs/patric_prod/patric_contrastive_650m_gradacc
  num_train_epochs: 10
  per_device_train_batch_size: 2 # Smaller batch size than the other runs
  per_device_eval_batch_size: 2
  gradient_accumulation_steps: 2 # Use gradient accumulation to get same number of steps as batch size 4
  dataloader_num_workers: 8
  eval_steps: 500 # More frequent logging than the other runs
  logging_steps: 500
  save_steps: 500
  label_names: ["labels"]

compute_codon_loss: true
compute_aminoacid_loss: true
compute_contrastive_loss: true
contrastive_temperature: 0.1
contrastive_pooler: mean
base_model: facebook/esm2_t33_650M_UR50D
tokenizer_path: /lus/eagle/projects/CVD-Mol-AI/braceal/src/genslm-esm/tokenizer_esm_genslm
train_path: /lus/eagle/projects/candle_aesp/hippekp/genslm-curriculum-datasets/homology_dataset/patric50/pgfam_30k_homology50_curriculum.h5
eval_path: /lus/eagle/projects/candle_aesp/hippekp/genslm-curriculum-datasets/homology_dataset/patric50/pgfam_30k_homology50_curriculum.h5
sequence_homology_path: /lus/eagle/projects/candle_aesp/hippekp/genslm-curriculum-datasets/homology_dataset/patric50/cluster_membership_to_h5.npy
num_eval_samples_per_epoch: 60000
wandb_project: genslm-esm-patric-experiments